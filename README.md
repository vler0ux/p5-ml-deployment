<<<<<<< feature/cicd-pipeline
# API Attrition RH ‚Äî Projet P5

D√©ploiement d'un mod√®le de machine learning pour pr√©dire le risque de d√©part des employ√©s.

## üìã Description

Ce projet expose un mod√®le de **R√©gression Logistique** entra√Æn√© sur des donn√©es RH via une API REST FastAPI. Chaque pr√©diction est enregistr√©e dans une base de donn√©es PostgreSQL pour assurer une tra√ßabilit√© compl√®te.
=======
---
title: P5 ML Deployment
emoji: ü§ñ
colorFrom: blue
colorTo: green
sdk: docker
pinned: false
---

# P5 - D√©ploiement ML - Attrition RH

API de pr√©diction d'attrition des employ√©s d√©velopp√©e avec FastAPI.
EOF

# API Attrition RH ‚Äî Projet P5

D√©ploiement d'un mod√®le de Machine Learning pour pr√©dire le risque de d√©part des employ√©s chez Futurisys.

##  Description

Ce projet expose un mod√®le de **R√©gression Logistique** via une API REST FastAPI. Chaque pr√©diction est enregistr√©e dans une base PostgreSQL pour assurer une tra√ßabilit√© compl√®te des interactions.

**Mod√®le** : Logistic Regression (scikit-learn) avec StandardScaler  
**Objectif** : Pr√©dire si un employ√© va quitter l'entreprise (classification binaire)  
**Dataset** : 1470 employ√©s, 37 features
>>>>>>> develop

## Architecture

```
p5/
‚îú‚îÄ‚îÄ api/
<<<<<<< feature/cicd-pipeline
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # API FastAPI
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ create_db.py     # Cr√©ation des tables
‚îÇ   ‚îú‚îÄ‚îÄ insert_data.py   # Insertion du dataset
‚îÇ   ‚îî‚îÄ‚îÄ db.py            # Connexion SQLAlchemy
‚îú‚îÄ‚îÄ models/              # Fichiers .joblib (non versionn√©s)
‚îú‚îÄ‚îÄ notebooks/           # Notebook d'entra√Ænement
‚îú‚îÄ‚îÄ tests/               # Tests unitaires
‚îî‚îÄ‚îÄ .github/workflows/   # CI/CD
```

## Installation

### Pr√©requis
- Python 3.11+
- PostgreSQL
- uv

### √âtapes

```bash
# Cloner le repo
git clone https://github.com/vler0ux/p5-ml-deployment.git
cd p5-ml-deployment

# Installer les d√©pendances
uv install

# Configurer la base de donn√©es
sudo -u postgres psql
CREATE DATABASE attrition_db;
CREATE USER attrition_user WITH PASSWORD 'attrition_pass';
GRANT ALL PRIVILEGES ON DATABASE attrition_db TO attrition_user;
\q

# Cr√©er les tables
uv run python database/create_db.py

# Ins√©rer le dataset
uv run python database/insert_data.py

# G√©n√©rer le mod√®le (ex√©cuter le notebook)
cd notebooks && uv run jupyter lab
=======
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # API FastAPI
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ create_db.py         # Cr√©ation des tables
‚îÇ   ‚îú‚îÄ‚îÄ insert_data.py       # Insertion du dataset
‚îÇ   ‚îî‚îÄ‚îÄ db.py                # Connexion SQLAlchemy
‚îú‚îÄ‚îÄ models/                  # Fichiers .joblib (non versionn√©s)
‚îú‚îÄ‚îÄ notebooks/               # Notebook d'entra√Ænement
‚îú‚îÄ‚îÄ tests/                   # Tests unitaires pytest
‚îú‚îÄ‚îÄ .env.example             # Template des variables d'environnement
‚îî‚îÄ‚îÄ .github/workflows/       # CI/CD GitHub Actions
```

##  Installation

### Pr√©requis
- Python 3.11+
- PostgreSQL 16+
- uv (gestionnaire de paquets)


### √âtapes
```bash
# 1. Cloner le repo
git clone https://github.com/TON_USERNAME/p5-ml-deployment.git
cd p5-ml-deployment

# 2. Installer les d√©pendances
uv install

# 3. Configurer les variables d'environnement
cp .env.example .env
# √âditer .env avec vos valeurs

# 4. Configurer PostgreSQL
sudo -u postgres psql
```
```sql
CREATE DATABASE attrition_db;
CREATE USER attrition_user WITH PASSWORD 'votre_mot_de_passe';
GRANT ALL PRIVILEGES ON DATABASE attrition_db TO attrition_user;
GRANT ALL ON SCHEMA public TO attrition_user;
\q
```

```bash
# 5. Cr√©er les tables
uv run python database/create_db.py

# 6. Ins√©rer le dataset
uv run python database/insert_data.py

# 7. G√©n√©rer le mod√®le (ex√©cuter le notebook)
uv run jupyter lab notebooks/02_modelisation.ipynb
>>>>>>> develop
```

## Lancement

```bash
uv run uvicorn api.main:app --reload
```

<<<<<<< feature/cicd-pipeline
L'API est disponible sur `http://localhost:8000`
La documentation Swagger sur `http://localhost:8000/docs`

## Endpoints

| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/` | Message de bienvenue |
| GET | `/health` | Statut de l'API |
| POST | `/predict` | Pr√©diction de d√©part |
=======
- API : `http://localhost:8000`
- Documentation Swagger : `http://localhost:8000/docs`

## üì°Endpoints

| M√©thode | Endpoint | Auth | Description |
|---------|----------|------|-------------|
| GET | `/` | ‚ùå | Message de bienvenue |
| GET | `/health` | ‚ùå | Statut de l'API |
| POST | `/predict` | ‚úÖ | Pr√©diction de d√©part |

##  Authentification

L'endpoint `/predict` est prot√©g√© par une **API Key**.

Ajoute le header suivant √† chaque requ√™te :
```
X-API-Key: votre_cle_api
```

Dans Swagger, clique sur le cadenas üîí en haut √† droite et entre ta cl√©.

## S√©curit√©

- Les secrets (API Key, mot de passe BDD) sont stock√©s dans `.env` (jamais versionn√©)
- `.env.example` documente les variables n√©cessaires sans exposer les valeurs
- Les fichiers `.joblib` ne sont pas versionn√©s (trop lourds et r√©g√©n√©rables)
- L'acc√®s √† la BDD est limit√© √† un utilisateur d√©di√© avec droits restreints
>>>>>>> develop

## Exemple d'utilisation

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
<<<<<<< feature/cicd-pipeline
=======
  -H "X-API-Key: votre_cle_api" \
>>>>>>> develop
  -d '{
    "age": 28,
    "revenu_mensuel": 3000,
    "heure_supplementaires": "Oui",
    "satisfaction_employee_environnement": 1,
    "frequence_deplacement": "Frequent"
  }'
```

R√©ponse :
<<<<<<< feature/cicd-pipeline

=======
>>>>>>> develop
```json
{
  "prediction": 1,
  "label": "Risque de d√©part",
  "probabilite_depart": 0.74
}
```

<<<<<<< feature/cicd-pipeline
## Base de donn√©es

- **employes** : dataset complet (1470 lignes)
- **predictions** : historique de toutes les pr√©dictions

### Processus de stockage
Chaque appel √† `/predict` enregistre automatiquement les inputs et outputs dans la table `predictions` via SQLAlchemy, assurant une tra√ßabilit√© compl√®te.

## üîê Authentification et gestion des acc√®s

### M√©thode actuelle ‚Äî API Key

L'endpoint `/predict` est prot√©g√© par une cl√© API transmise dans le header HTTP :
```
X-API-Key: votre_cle_api
```

Sans cl√© valide, l'API retourne une erreur `403 Forbidden`.

### Pour aller plus loin en production

| Besoin | Solution recommand√©e |
|--------|---------------------|
| Utilisateurs multiples | JWT (JSON Web Tokens) avec OAuth2 |
| Gestion des r√¥les | RBAC (Role-Based Access Control) |
| Expiration des tokens | JWT avec dur√©e de vie limit√©e |
| Audit des acc√®s | Logs des requ√™tes avec identifiant utilisateur |

## Bonnes pratiques de s√©curit√©

### Gestion des secrets

Les secrets ne sont **jamais versionn√©s** dans Git :

```bash
# Bonne pratique
API_KEY=xxx        ‚Üí stock√© dans .env (ignor√© par .gitignore)

# √Ä ne jamais faire
API_KEY=xxx        ‚Üí √©crit directement dans le code
```

En production, utiliser un gestionnaire de secrets :

- **GitHub Actions** : secrets chiffr√©s dans les Settings du repo
- **Production** : HashiCorp Vault, AWS Secrets Manager, etc.

### Hachage de mot de passe

Si le projet √©volue vers un syst√®me avec comptes utilisateurs, les mots de passe ne doivent **jamais √™tre stock√©s en clair**. Utiliser `bcrypt` :
```python
from passlib.context import CryptContext

pwd_context = CryptContext(schemes=["bcrypt"])

# Hacher le mot de passe avant stockage
hashed = pwd_context.hash("mot_de_passe")

# V√©rifier lors de la connexion
pwd_context.verify("mot_de_passe", hashed)
```

### Acc√®s √† la base de donn√©es

- Un utilisateur PostgreSQL d√©di√© (`attrition_user`) avec droits limit√©s
- Les credentials BDD stock√©s dans `.env`
- Aucun acc√®s root en production

### Variables d'environnement requises
```bash
# .env.example
API_KEY=votre_cle_api_ici
DATABASE_URL=postgresql://user:password@localhost/attrition_db
```
=======
##  Base de donn√©es

### Structure des tables

**Table `employes`** : dataset complet (1470 lignes)
| Colonne | Type | Description |
|---------|------|-------------|
| id | INTEGER | Cl√© primaire |
| age | INTEGER | √Çge de l'employ√© |
| revenu_mensuel | FLOAT | Salaire mensuel |
| departement | VARCHAR | D√©partement |
| a_quitte_l_entreprise | VARCHAR | Valeur r√©elle (Oui/Non) |

**Table `predictions`** : historique des pr√©dictions
| Colonne | Type | Description |
|---------|------|-------------|
| id | INTEGER | Cl√© primaire |
| date_prediction | DATETIME | Horodatage |
| age | INTEGER | √Çge soumis |
| prediction | INTEGER | 0=stable, 1=d√©part |
| probabilite_depart | FLOAT | Score de probabilit√© |

### Processus de stockage
Chaque appel √† `/predict` enregistre automatiquement les inputs et outputs dans la table `predictions` via SQLAlchemy, assurant une tra√ßabilit√© compl√®te.

## Tests

```bash
uv run pytest tests/ -v --cov=api --cov-report=html
```

Le rapport de couverture est g√©n√©r√© dans `htmlcov/`.
>>>>>>> develop

## CI/CD

Le pipeline GitHub Actions (`.github/workflows/ci.yml`) :
- S'ex√©cute √† chaque push
- Lance les tests automatiquement
- G√®re les environnements dev et prod via les secrets GitHub

## Stack technique

| Outil | Usage |
|-------|-------|
| FastAPI | API REST |
| Pydantic | Validation des donn√©es |
| scikit-learn | Mod√®le ML |
| PostgreSQL | Base de donn√©es |
| SQLAlchemy | ORM |
| uv | Gestionnaire de paquets |
| pytest | Tests unitaires |
| GitHub Actions | CI/CD |